{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, models, metrics\n",
    "\n",
    "import numpy as np \n",
    "import scipy as sp \n",
    "import pandas as pd\n",
    "from  statsmodels.tsa.api import acf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "import midiio\n",
    "import frequency_analysis as frqa\n",
    "from midi.midi_representation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIDI_START = 20\n",
    "MIDI_END = 127\n",
    "\n",
    "FREQUENCY_COUNT = 257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define our input and output data\n",
    "input_columns = [\"f\" + str(i) for i in range(0, FREQUENCY_COUNT)]\n",
    "label_columns = [i for i in range(MIDI_START, MIDI_END+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(wav_path, csv_path):\n",
    "    # Open the midifile to see what is inside. Also open the accopmanying labels.\n",
    "    data, sample_rate = midiio.read_file(wav_path)\n",
    "\n",
    "    music_dataframe = pd.read_csv(csv_path)\n",
    "\n",
    "    music_dataframe\n",
    "    WINDOW_SAMPLES = sample_rate * 0.05\n",
    "\n",
    "    # For the model, construct the training data as follows:\n",
    "    music_dataframe_copy = music_dataframe.copy()\n",
    "    training_data = pd.DataFrame(columns = ['start_time'] + label_columns + input_columns)\n",
    "\n",
    "    # Create a linear series where points are WINDOW_SAMPLES apart from each other. \n",
    "    serialized_data = []\n",
    "    xs = np.linspace(0, len(data), int (len(data) / WINDOW_SAMPLES), endpoint=False)\n",
    "\n",
    "    # Iterate over the music dataframe. Construct a one hot encoded vector for this particular time based on the note value\n",
    "    # At the given time.\n",
    "    i = 0\n",
    "\n",
    "    for x in xs:\n",
    "        notes_on = music_dataframe_copy.query(\"start_time <= \" + str(int(x))).query(\"end_time >= \" + str(int(x)))\n",
    "        music_dataframe_copy.drop(notes_on.index, axis='index', inplace=True)\n",
    "        \n",
    "        note_vec = [0 for i in range(MIDI_START, MIDI_END + 1)]\n",
    "        if len(notes_on) != 0:\n",
    "            for n in notes_on['note']:\n",
    "                note_vec[n - MIDI_START] = 1\n",
    "        \n",
    "        \n",
    "        f, power = frqa.get_frequencies(data[int(x) : int(x + WINDOW_SAMPLES)], sample_rate)\n",
    "        training_data.loc[len(training_data.index)] = [x, *note_vec, *np.abs(power)]\n",
    "\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = get_data(\"../data/musicnet/1727.wav\", \"../data/musicnet/1727.csv\").append(\n",
    "                get_data(\"../data/musicnet/1759.wav\", \"../data/musicnet/1759.csv\"))\n",
    "training_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:50:36) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86ffb8cd15e50cb3c3e2c8640421c5f15b79eb6c760510113e4a090e97faea88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
